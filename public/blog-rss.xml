<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/">
  <channel>
    <title>Blog Entries :: phly, boy, phly</title>
    <description>Blog Entries :: phly, boy, phly</description>
    <pubDate>Wed, 13 Feb 2013 13:40:00 +0000</pubDate>
    <generator>Zend_Feed_Writer 2.1.1dev (http://framework.zend.com)</generator>
    <link>http://mwop.net/blog.html</link>
    <atom:link rel="self" type="application/rss+xml" href="http://mwop.net/blog-rss.xml"/>
    <item>
      <title>RESTful APIs with ZF2, Part 2</title>
      <pubDate>Wed, 13 Feb 2013 13:40:00 +0000</pubDate>
      <link>http://mwop.net/blog/2013-02-13-restful-apis-with-zf2-part-2.html</link>
      <guid>http://mwop.net/blog/2013-02-13-restful-apis-with-zf2-part-2.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    In my <a href="/blog/2013-02-11-restful-apis-with-zf2-part-1.html">last post</a>,
    I covered some background on REST and the Richardson Maturity Model, and some
    emerging standards around hypermedia APIs in JSON; in particular, I outlined
    aspects of Hypermedia Application Language (HAL), and how it can be used to
    define a generic structure for JSON resources.
</p>

<p>
    In this post, I cover an aspect of RESTful APIs that's often overlooked:
    reporting problems.
</p><h2>Background</h2>

<p>
    APIs are useful when they're working. But when they fail, they're only 
    useful if they provide us with meaningful information; if all I get is
    a status code, and no indication of what caused the issue, or where I might
    look for more information, I get frustrated.
</p>

<p>
    In consuming APIs, I've come to the following conclusions:
</p>

<ul>
    <li>
        Error conditions need to provide detailed information as to what went 
        wrong, and what steps I may be able to take next. An error code with
        no context gives me nothing to go on.
    </li>

    <li>
        Errors need to be reported consistently. Don't report the error one way
        one time, and another way the next.
    </li>

    <li>
        <strong>DO</strong> use HTTP status codes to indicate an error happened.
        Nothing is more irksome than getting back a 200 status with an error 
        payload.
    </li>

    <li>
        Errors should be reported in a format I have indicated I will Accept 
        (as in the HTTP header). Perhaps the only think more irksome than a 200
        status code for an error is getting back an HTML page when I expect 
        JSON.
    </li>
</ul>

<h2>Why Status Codes Aren't Enough</h2>

<p>
    Since REST leverages and builds on HTTP, an expedient solution for reporting
    problems is to simply use <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP status codes</a>. 
    These are well understood by web developers, right?
</p>

<p>
    <code>4xx</code> error codes are errors made by the requestor, and are actually fairly 
    reasonable to use for reporting things such as lack of authorization tokens,
    incomplete requests, unsupportable operations, or non-supported media types.
</p>

<p>
    But what happens when the error is on the server - because something has
    gone wrong such as inability to reach your persistence layer or credential
    storage? The <code>5xx</code> series of status codes is sparse and wholly unsuited to
    reporting errors of these types -- <em>though you'll likely still want to use
    a <code>500</code> status to report the failure</em>. But what do you present to the consumer
    so that they know whether or not to try again, or what to report to you
    so that you can fix the issue?
</p>

<p>
    A status code simply isn't enough information most of the time. Yes, you 
    want to define standard status codes so that your clients can perform
    reasonable branching, but you also need a way to communicate <em>details</em>
    to the end-user, so that they can log the information for themselves, display
    information to their own end-users, and/or report it back to you so you can
    do something to resolve the situation.
</p>

<h2>Custom Media Types</h2>

<p>
    The first step is to use a custom media type. Media types are typically 
    both a name as well as a structure -- and the latter is what we're after
    when it comes to error reporting. 
</p>

<p>
    If we return a response using this media type, the client then knows how
    to parse it, and can then process it, log it, whatever.
</p>

<p>
    Sure, you can make up your own format -- as long as you are consistent
    in using it, and you document it. But personally, I don't like inventing
    new formats when standard formats exist already. Custom formats mean that 
    custom clients are required for working with the services; using a standard
    format can save effort and time.
</p>

<p>
    In the world of JSON, I've come across two error media types that appear to
    be gaining traction: <code>application/api-problem+json</code> and 
    <code>application/vnd.error+json</code>
</p>

<h3>API-Problem</h3>

<p>
    This particular media type is <a 
    href="http://tools.ietf.org/html/draft-nottingham-http-problem-02">via the 
    IETF</a>. Like HAL, it provides formats in both JSON and XML, making it 
    a nice cross-platform choice.
</p>

<p>
    As noted already, the media type is <code>application/api-problem+json</code>.
    The representation is a single resource, with the following properties:
</p>

<ul>
    <li>
        <strong>describedBy</strong>: a URL to a document describing the error condition (required)
    </li>

    <li>
        <strong>title</strong>: a brief title for the error condition (required)
    </li>

    <li>
        <strong>httpStatus</strong>: the HTTP status code for the current request (optional)
    </li>

    <li>
        <strong>detail</strong>: error details specific to this request (optional)
    </li>

    <li>
        <strong>supportId</strong>: a URL to the specific problem occurrence (e.g., to a log message) (optional)
    </li>
</ul>

<p>
    As an example:
</p>

<div class="example"><pre><code language="http">
HTTP/1.1 500 Internal Error
Content-Type: application/api-problem+json

{
    "describedBy": "http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html",
    "detail": "Status failed validation",
    "httpStatus": 500,
    "title": "Internal Server Error"
}
</code></pre></div>

<p>
    The specification allows a large amount of flexibility -- you can have your 
    own custom error types, so long as you have a description of them to link 
    to. You can provide as little or as much detail as you want, and even 
    decide what information to expose based on environment.
</p>

<p>
    I personally like to point to the HTTP status code definitions, and then 
    provide request-specific detail; I find this gives quick and simple results 
    that I can later shape as I add more detail to my API. However, the 
    specification definitely encourages you to have unique error types with 
    discrete URIs that describe them -- never a bad thing when creating APIs.
</p>

<h3>vnd.error</h3>

<p>
    This is a <a href="https://github.com/blongden/vnd.error">proposed media 
    type</a> within the HAL community. Like HAL, it provides formats in both 
    JSON and XML, making it a nice cross-platform choice.
</p>

<p>
    It differentiates from API-Problem in a few ways. First, it allows, and 
    even encourages, reporting collections of errors. If you consider PHP 
    exceptions and the fact that they support "previous" exceptions, this is a 
    powerful concept; you can report the entire chain of errors that led to the 
    response.  Second, it encourages pushing detail out of the web service; 
    errors include a "logRef" property that points to where the error detail lives. 
    This is probably better illustrated than explained.
</p>

<p>
    The response payload is an array of objects. Each object has the following 
    members:
</p>

<ul>
    <li>
        <strong>logRef</strong>: a unique identifier for the specific error which can then be
        used to identify the error within server-side logs (required)
    </li>

    <li>
        <strong>message</strong>: the error message itself (required)
    </li>

    <li>
        <strong>_links</strong>: HAL-compatible links. Typically, "help", "describes", and/or 
        "describedBy" relations will be defined here.
    </li>
</ul>

<p>
    As an example, let's consider the API-Problem example I had earlier, and
    provide a vnd.error equivalent:
</p>

<div class="example"><pre><code language="http">
HTTP/1.1 500 Internal Error
Content-Type: application/vnd.error+json

[
    {
        "logRef": "someSha1HashMostLikely",
        "message": "Status failed validation",
        "_links": {
            "describedBy": {"href": "http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"}
        }
    }
]
</code></pre></div>

<p>
    vnd.error basically begs you to create custom error types, with documentation 
    end-points that detail the source of the error and what you can do about it 
    (this is true of API-Problem as well).
</p>

<p>
    The requirement to include a log reference ("logRef") and have it be unique 
    can be a stumbling block to implementation, however, as it requires effort 
    for uniquely identifying requests, and logging. However, both the 
    identification and logging can be automated.
</p>

<h2>Summary</h2>

<p>
    Error reporting in APIs is as important as the normal resource payloads 
    themselves. Without good error reporting, when an API raises errors, 
    clients have difficulty understanding what they can do next, and cannot
    provide you, the API provider, with information that will allow you to
    debug on the server side.
</p>

<p>
    As noted at the beginning of the article, if you follow the rules below,
    you'll make consumers of your API happier and more productive.
</p>

<ul>
    <li>
        <strong>DO</strong> use appropriate HTTP status codes to indicate an 
        error happened.
    </li>

    <li>
        Report errors in a format I have indicated I will Accept 
        (as in the HTTP header). 
    </li>
    <li>
        Report errors consistently. Don't report the error one way one time, 
        and another way the next. Standardize on a specific error-reporting 
        media type .  While you <em>can</em> create your own error structure, I 
        recommend using documented, accepted standards. This will make clients 
        more re-usable, and make many of your decisions for you.
    </li>

    <li>
        Provide detailed information as to what went wrong, and what steps I 
        may be able to take next. Provide documentation for each type of error, 
        and link to that documentation from your error payloads.
    </li>

</ul>

<p>
    Which brings me to...
</p>

<h2>Next time</h2>

<p>
    I realize I still haven't covered anything specific to ZF2, but I'll start 
    next time, when I cover the next topic: documenting your API. An 
    undocumented API is a useless API, so it's good to start baking 
    documentation in immediately. I'll survey some of the possibilities and how 
    they can be implemented in ZF2 in the next installment, and then we can get 
    our hands dirty with actual API development.
</p>

<h3>Updates</h3>

<p>
    <em>Note: I'll update this post with links to the other posts in the series 
    as I publish them.</em>
</p>

<ul>
    <li><a href="/blog/2013-02-11-restful-apis-with-zf2-part-1">Part 1</a></li>
</ul>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>RESTful APIs with ZF2, Part 1</title>
      <pubDate>Wed, 13 Feb 2013 13:40:00 +0000</pubDate>
      <link>http://mwop.net/blog/2013-02-11-restful-apis-with-zf2-part-1.html</link>
      <guid>http://mwop.net/blog/2013-02-11-restful-apis-with-zf2-part-1.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    RESTful APIs have been an interest of mine for a couple of years, but due
    to <a href="http://framework.zend.com/blog//zend-framework-2-0-0-stable-released.html">circumstances</a>,
    I've not had much chance to work with them in any meaningful fashion until
    recently.
</p>

<p>
    <a href="http://akrabat.com/">Rob Allen</a> and I proposed a workshop for
    <a href="http://conference.phpbenelux.eu/2013/">PHP Benelux 2013</a> 
    covering RESTful APIs with ZF2. When it was accepted, it gave me the perfect
    opportunity to dive in and start putting the various pieces together.
</p><h2>Background</h2>

<p>
    I've attended any number of conference sessions on API design, read 
    countless articles, and engaged in quite a number of conversations. Three
    facts keep cropping up:
</p>

<ol>
    <li>JSON is fast becoming the preferred exchange format due to the ease 
    with which it de/serializes in almost every language.</li>
    <li>The "holy grail" is <a 
    href="http://martinfowler.com/articles/richardsonMaturityModel.html">Richardson 
    Maturity Model</a> Level 3.</li>
    <li>It's really hard to achieve RMM level 3 with JSON.</li>
</ol>

<h3>Richardson Maturity Model</h3>

<p>
    As a quick review, the Richardson Maturity Model has the following 4 levels:
</p>

<ul>
    <li>Level 0: "The swamp of POX." Basically, a service that uses TCP for 
        transport, primarily as a form of remote procedure call (RPC). 
        Typically, these are not really leveraging HTTP in any meaningful 
        fashion; most systems will use HTTP POST for all interactions. Also, 
        you will often have a single endpoint for all interactions, regardless 
        of whether or not they are strictly related. XML-RPC, SOAP, and 
        JSON-RPC fall under this category.
    </li>

    <li>Level 1: "Resources." In these services, you start breaking the service
        into multiple services, one per "resource," or, in object oriented terms,
        per object. This means a distinct URL per object, which means each
        has its own distinct identity on the web; this often extends not only 
        to the collection of objects, but to individual objects under the 
        collection as well (e.g., "/books" as well as "/books/life-of-pi"). The 
        service may still be RPC in nature, however, and, at this level, often 
        is still using a single HTTP method for all interactions with the 
        resource.
    </li>

    <li>Level 2: "HTTP Verbs." At this level, we start using HTTP verbs with
        our services in the way the HTTP specification intends. GET is for safe 
        operations, and should be cacheable; POST is used for creation and/or 
        updating; DELETE can be used to delete a resource; etc. Rather than 
        doing RPC style methods, we leverage HTTP, occasionally passing 
        additional parameters via the query string or request body.  
        Considerations such as HTTP caching and idempotence are taken into 
        account.
    </li>

    <li>Level 3: "Hypermedia Controls." Building on the previous level, our
        resource representations now also include <em>links</em>, which indicate
        what we can <em>do next</em>. At this level, our API becomes practically
        self-describing; given a single end-point, we should be able to start
        crawling it, using the links in a representation to lead us to the next
        actions.
    </li>
</ul>

<p>
    When I first started playing with web services around a decade ago, 
    everything was stuck at Level 0 or Level 1 -- usually with Level 1 users 
    downgrading to Level 0 because Level 0 offerred consistency and 
    predictability if you chose to use a service type that had a defined 
    envelope format (such as XML-RPC or SOAP).  (I even wrote the XML-RPC 
    server implementation for Zend Framework because I got sick of writing 
    one-off parsers/serializers for custom XML web service implementations.
    When you're implementing many services, predictability is a huge win.)
</p>

<p>
    A few years ago, I started seeing a trend towards Level 2. Web developers
    like the simplicity of using HTTP verbs, as they map very well to <a 
    href="http://en.wikipedia.org/wiki/Create,_read,_update_and_delete">CRUD</a>
    operations -- the bread and butter of web development. Couple this concept
    with JSON, and it becomes trivially simple to both create a web service, as
    well as consume it.
</p>

<p>
    <em>I'd argue that the majority of web developers are quite happy to be at 
    Level 2 -- and have no problem staying there. They're productive, and the 
    concepts are easy -- both to understand and to implement.</em>
</p>

<p>
    Level 3, though, is where it becomes really interesting. The idea that
    I can examine the represention <em>alone</em> in order to understand what
    I can do next is very intriguing and empowering.
</p>

<h3>JSON and Hypermedia</h3>

<p>
    With XML, hypermedia basically comes for free. Add some &lt;link&gt; 
    elements to your representation, and you're done -- and don't forget the 
    link <code>rel</code>ations!
</p>

<p>
    JSON, however, is another story.
</p>

<p>
    Where do the links go? <em>There is no single, defined way to represent a 
    hyperlink in JSON.</em>
</p>

<p>
    Fortunately, there are some emerging standards. 
</p>

<p>
    First is use of the <a href="http://www.w3.org/wiki/LinkHeader">"Link" HTTP 
    header</a>. While the page I linked shows only a single link in the
    header, you can have multiple links separated by commas. GitHub uses this
    when providing pagination links in their API. Critics will point out that
    the HTTP headers are not technically part of the representation, however;
    strict interpetations of REST and RMM indicate that the hypermedia links
    should be part of the resource representation. Regardless, having the links
    in the HTTP headers is useful for pre-traversal of a service, as you can
    perform HEAD requests only to discover possible actions and workflows.
</p>

<p>
    <a 
    href="http://amundsen.com/media-types/collection/format/">Collection+JSON</a>
    is interesting, as it describes the entire JSON envelope. My one criticism
    is that it details too much; whenever I see a format that dictates how to 
    describe types, I think of XML-RPC or SOAP, and get a little twitchy. It's
    definitely worth a look, though.
</p>

<p>
    What's captured my attention of late, however, is 
    <a href="http://stateless.co/hal_specification.html">Hypertext Application Language</a>,
    or HAL for short. HAL has very few rules, but succinctly describes both how
    to provide hypermedia in JSON as well as how to represent embedded resources
    - the two things that most need standardized structure in JSON. It does this
    while still providing a generic media type, and also describing a mirror 
    image XML format!
</p>

<h3>HAL Media Types</h3>

<p>
    HAL defines two generic media types: <code>application/hal+xml</code> and
    <code>application/hal+json</code>. You will use these as the response
    Content-Type, as they describe the response representation; the client
    can simply request <code>application/json</code>, and the response
    format remains compatible.
</p>

<h3>HAL and Links</h3>

<p>
    HAL provides a very simple structure for JSON hypermedia links. First, all 
    resource representations must contain hypermedia links, and all links are 
    provided in a "_links" object:
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
    }
}
</code></pre></div>

<p>
    Second, links are properties of this object. The property name is the link
    relation, and the value is an object containing minimally an "href" 
    property.
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
        "self": {"href": "http://example.com/api/status/1234"}
    }
}
</code></pre></div>

<p>
    If a given relation can have multiple links, you provide instead an array
    of objects:
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
        "self": {"href": "http://example.com/api/status/1234"},
        "conversation": [
            {"href": "http://example.com/api/status/1237"},
            {"href": "http://example.com/api/status/1241"}
        ]
    }
}
</code></pre></div>

<p>
    Individual links can contain other attributes as desired -- I've seen
    people include the relation again so that it's self-contained in the link 
    object, and it's not uncommon to include a title or name.
</p>

<h3>HAL and Resources</h3>

<p>
    HAL imposes no structure over resources other than requiring the hypermedia 
    links; even then, you typically do not include the hypermedia links when 
    making a request of the web service; the hypermedia links are included only 
    in the representations <em>returned</em> by the service.
</p>

<p>
    So, as an example, you would POST the following:
</p>

<div class="example"><pre><code language="http">
POST /api/status
Host: example.com
Accept: application/json
Content-Type: application/json

{
    "status": "This is my awesome status update!",
    "user": "mwop"
}
</code></pre></div>

<p>
    And from that request, you'd receive the following:
</p>

<div class="example"><pre><code language="http">
201 Created
Location: http://example.com/api/status/1347
Content-Type: application/hal+json

{
    "_links": {
        "self": {"href": "http://example.com/api/status/1347"}
    },
    "id": "1347",
    "timestamp": "2013-02-11 23:33:47",
    "status": "This is my awesome status update!",
    "user": "mwop"
}
</code></pre></div>

<h3>HAL and Embedded Resources</h3>

<p>
    The other important thing that HAL defines is how to <em>embed</em>
    resources. Why is this important? If the resource references other
    resources, you will want to be able to link to them so you can perform
    operations on them, too.
</p>

<p>
    Embedded resources are represented inside an "_embedded" object of the
    representation, and, as resources, contain their own "_links" object as 
    well. Each resource you embed is assigned to a property of that
    object, and if multiple objects of the same type are returned, an array
    of resources is assigned. In fact, this latter is how you represent
    <em>collections</em> in HAL.
</p>

<p>
    Let's consider a simple example first. In previous code samples, I have
    a "user" that's a string; let's make that an embedded resource instead.
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
        "self": {"href": "http://example.com/api/status/1347"}
    },
    "id": "1347",
    "timestamp": "2013-02-11 23:33:47",
    "status": "This is my awesome status update!",
    "_embedded": {
        "user": {
            "_links": {
                "self": {"href": "http://example.com/api/user/mwop"}
            }
            "id": "mwop",
            "name": "Matthew Weier O'Phinney",
            "url": "http://mwop.net"
        }
    }
}
</code></pre></div>

<p>
    I've moved the "user" out of the representation, and into the "_embedded"
    object -- because this is where you define embedded resources. Note that
    the "user" is a standard HAL resource itself -- containing hypermedia links.
</p>

<p>
    Now let's look at a collection:
</p>

<div class="example"><pre><code language="javascript">
{
    "_links": {
        "self": {"href": "http://example.com/api/status"},
        "next": {"href": "http://example.com/api/status?page=2"},
        "last": {"href": "http://example.com/api/status?page=100"}
    },
    "count": 2973,
    "per_page": 30,
    "page": 1,
    "_embedded": {
        "status": [
            {
                "_links": {
                    "self": {"href": "http://example.com/api/status/1347"}
                },
                "id": "1347",
                "timestamp": "2013-02-11 23:33:47",
                "status": "This is my awesome status update!",
                "_embedded": {
                    "user": {
                        "_links": {
                            "self": {"href": "http://example.com/api/user/mwop"}
                        }
                        "id": "mwop",
                        "name": "Matthew Weier O'Phinney",
                        "url": "http://mwop.net"
                    }
                }
            }
            /* ... */
        ]
    }
}
</code></pre></div>

<p>
    Note that the "status" property is an array; semantically, all resources
    under this key are of the same type. Also note that the parent resource
    has some additional link relations -- these are related to pagination, and
    allow a client to determine what the next and last pages are (and, if we 
    were midway into the collection, previous and first pages). Since the 
    collection is also a resource, it has some interesting metadata -- how
    many resources are in the collection, how many we represent per page, and
    what the current page is.
</p>

<p>
    Also note that you can nest resources -- simply include an "_embedded" 
    object inside an embedded resource, with additional resources, as I've
    done with the "user" resource inside the status resource shown here. It's 
    turtles all the way down.
</p>

<h2>Next Time</h2>

<p>
    The title of this post indicates I'll be talking about building RESTful 
    APIs with ZF2 -- but so far, I've not said anything about ZF2.
</p>

<p>
    I'll get there. But there's another detour to take: reporting errors.
</p>

<h3>Updates</h3>

<p>
    <em>Note: I'll update this post with links to the other posts in the series 
    as I publish them.</em>
</p>

<ul>
    <li><a href="/blog/2013-02-13-restful-apis-with-zf2-part-2">Part 2</a></li>
</ul>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>OpenShift, Cron, and Naked Domains</title>
      <pubDate>Sun, 30 Dec 2012 15:52:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-12-30-openshift-cron-and-naked-domains.html</link>
      <guid>http://mwop.net/blog/2012-12-30-openshift-cron-and-naked-domains.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    As an experiment, I migrated my website over to <a 
    href="http://openshift.redhat.com">OpenShift</a> yesterday. I've been hosting
    a pastebin there already, and have found the service to be both straightforward
    and flexible; it was time to put it to a more thorough test.
</p>

<p>
    In the process, I ran into a number of interesting issues, some of which took quite
    some time to resolve; this post is both to help inform other potential users of the
    service, as well as act as a reminder to myself.
</p><h2>Cron</h2>

<p>
    OpenShift offers a <a href="http://en.wikipedia.org/wiki/Cron">Cron</a> cartridge,
    which I was excited to try out.<sup><a href="#f1">1</a></sup>
</p>

<p>
    The basics are quite easy. In your repository's <code>.openshift</code> 
    directory is a <code>cron</code> subdirectory, further divided into 
    <code>minutely</code>, <code>hourly</code>, <code>daily</code>, <code>weekly</code>,
    and <code>monthly</code> subdirectories. You drop a script you want to run
    into one of these directories, and push your changes upstream.
</p>

<p>
    The problem is: what if I want a job to run at a specific time daily? or on 
    the quarter hour? or on a specific day of the week?
</p>

<p>
    As it turns out, you can manage all of the above, just not quite as succinctly as
    you would in a normal crontab. Here, for example, is a script that I run at 
    5AM daily; I placed it in the <code>hourly</code> directory so that it can test
    more frequently:
</p>

<div class="example"><pre><code language="bash">
#!/bin/bash
if [ `date +%H` == "05" ]
then
    (
        export PHP=/usr/local/zend/bin/php ;
        cd $OPENSHIFT_REPO_DIR ; 
        $PHP public/index.php phlycomic fetch all ; 
        $PHP public/index.php phlysimplepage cache clear --page=pages/comics 
    )
fi
</code></pre></div>

<p>
    And here's one that runs on the quarter-hour, placed in the <code>minutely</code>
    directory:
</p>

<div class="example"><pre><code language="bash">
#!/bin/bash
MINUTES=`date +%M`

for i in "00" "15" "30" "45";do
    if [ "$MINUTES" == "$i" ];then
        (
            export PHP=/usr/local/zend/bin/php ;
            cd $OPENSHIFT_REPO_DIR ;
            $PHP public/index.php githubfeed fetch 
        )
    fi
done
</code></pre></div>

<p>
    The point is that if you need more specificity, push the script into the 
    next more specific directory, and test against the time of execution.
</p>

<h2>Naked Domains</h2>

<p>
    Naked domains are domains without a preceding subdomain. In my case, this
    means "mwop.net", vs. "www.mwop.net".
</p>

<p>
    The problem that cloud hosting presents is that the IP address on which you
    are hosted can change at any time, for a variety of reasons. As such, you
    typically cannot use DNS A records to point to your domain; the recommendation
    is to use a CNAME record that points the domain to a "virtual" domain 
    registered with your cloud hosting provider.
</p>

<p>
    However, most domain registrars and DNS providers do not let you do this for
    a naked domain, particularly if you also have MX or other records associated
    with that naked domain.
</p>

<p>
    Some registrars will allow you to forward the A record to a subdomain. I tried
    this, but had limited success; I personally found that I ended up in an infinite
    loop situation when doing the DNS lookup.
</p>

<p>
    Another solution is to have a redirect in place for your naked domain to the
    subdomain, which can then be a CNAME record. Typically, this would require you
    have a web server under your control with a fixed IP that then simply redirects
    to the subdomain. Fortunately, there's an easier solution: <a 
    href="http://wwwizer.com/naked-domain-redirect">wwwizer</a>. You simply point
    your naked domain A record to the wwwizer IP address, and they will do a 
    redirect to your <code>www</code> subdomain.
</p>

<p>
    I implemented wwwizer on my domain (which is why you'll see "www.mwop.net" in
    your location bar), and it's been working flawlessly since doing so.
</p>

<h4>Private repositories</h4>

<p>
    I keep my critical site settings in a private repository, which allows me 
    to version them while keeping the credentials they hold out of the public eye.
    This means, however, that I need to use <a href="https://help.github.com/articles/managing-deploy-keys">GitHub deploy keys</a> on my server
    in order to retrieve changes.
</p>

<p>
    This was simple enough: I created an <code>ssh</code> subdirectory in my
    <code>$OPENSHIFT_DATA_DIR</code> directory, and generated a new SSH keypair.
</p>

<p>
    The problem was telling SSH to <em>use</em> this key when fetching changes.
</p>

<p>
    The solution is to use a combination of <code>ssh-agent</code> and <code>ssh-add</code>,
    and it looks something like this:
</p>

<div class="example"><pre><code language="bash">
#!/bin/bash
ssh-agent `ssh-add $OPENSHIFT_DATA_DIR/ssh/github-key && (
    cd $OPENSHIFT_DATA_DIR/config ; 
    git fetch origin ; 
    git rebase origin/mwop.net.config
)`
</code></pre></div>

<p>
    After testing the above, I put this in a <code>pre_build</code> script in 
    my OpenShift configuration so that I can autoupdate my private 
    configuration on each build. However, I discovered a new problem: when
    a build is being done, the <code>ssh-agent</code> is not available, which
    means the above cannot be executed. I'm still trying to find a solution.
</p>

<h2>Fin</h2>

<p>
    I'm pretty happy with the move. I don't have to do anything special
    to automate deployment, and all my cronjobs and deployment scripts are now
    self-contained in the repository, which makes my site more portable.
    While a few things could use more documentation, all the pieces are there
    and discoverable with a small amount of work.
</p>

<p>
    I'll likely give some other PaaS providers a try in the future, but for 
    the moment, I'm quite happy with the functionality and flexibility of 
    OpenShift.
</p>

<h4>Footnotes</h4>

<ul>
    <li id="f1">Zend Server's JobQueue can also be used as a cron replacement, 
    but I was not keen on exposing the job functionality via HTTP.</li>
</ul>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>On php-fig and Shared Interfaces</title>
      <pubDate>Thu, 20 Dec 2012 20:23:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-12-20-on-shared-interfaces.html</link>
      <guid>http://mwop.net/blog/2012-12-20-on-shared-interfaces.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    This is a post I've been meaning to write for a long time, and one requested
    of me personally by <a href="http://www.rooftopsolutions.nl/blog/">Evert 
    Pot</a> during the Dutch PHP Conference in June 2012. It details some observations
    I have of php-fig, and hopefully will serve as a record of why I'm not
    directly participating any longer.
</p>

<p>
    I was a founding member of the <a href="http://www.php-fig.org/">Framework 
    Interoperability Group</a>, now called "php-fig". I was one of around a dozen 
    folks who sat around a table in 2009 in Chicago during php|tek and started 
    discussions about what we could all do to make it possible to work better 
    together between our projects, and make it simpler for users to pick and choose 
    from our projects in order to build the solutions to their own problems.
</p>

<p>
    The first "standard" that came from this was <a 
    href="https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-0.md">PSR-0</a>, 
    which promoted a standard class naming convention that uses a 1:1 relationship 
    between the namespace and/or vendor prefix and the directory hierarchy, and the 
    class name and the filename in which it lives. To this day, there are both 
    those who hail this as a great step forward for cooperation, and simultaneously 
    others who feel it's a terrible practice. 
</p>

<p>
    And then nothing, for years. But a little over a year ago, there was a new 
    push by a number of folks wanting to do more. Paul Jones did a remarkable 
    job of spearheading the next <a href="https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-1-basic-coding-standard.md">two</a> 
    <a href="https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-2-coding-style-guide.md">standards</a>, 
    which centered around coding style. Again, just like with PSR-0, we had 
    both those feeling it was a huge step forward, and those who loathe the 
    direction.
</p>

<p>
    What was interesting, though, was that once we started seeing some new energy
    and momentum, it seemed that <em>everyone</em> wanted a say. And we started 
    getting dozens of folks a week asking to be voting members, and new proposal
    after new proposal. Whether or not somebody likes an existing standard, they
    want to have backing for a standard they propose.
</p>

<p>
    And this is when we started seeing proposals surface for shared interfaces, first
    around caching, and now around logging (though the latter is the first up for
    vote).
</p><h2>Shared Interfaces</h2>

<p>
    The idea around shared interfaces is simple: if we can come to a consensus on
    the basic interface for a common application task, libraries and frameworks
    can typehint on that shared interface, allowing developers to drop in the 
    implementation of their choosing -- or even a standard, reference implementation.
    The goal is to prevent Not Invented Here (NIH) syndrome, as well as to make
    it simpler to re-use components between one library and another. As an example,
    if you're using Framework A, and it has a caching library, and you're consuming
    ORM B, you'd be able to pass the same cache object to the ORM as you use in the
    framework.
</p>

<p>
    Great goals, really.
</p>

<p>
    But I'm not sure I buy into them.
</p>

<h2>Problems</h2>

<p>
    First, I agree that NIH is a problem.
</p>

<p>
    Second, I <em>also</em> think there's space for <em>multiple 
    implementations</em> of any given component. Often there are different 
    approaches that different authors will take: one might focus on 
    performance, another on having multiple adapters for providing different 
    capabilities, etc. Sometimes having a different background will present 
    different problem areas you want to resolve. As such, having multiple 
    implementations can be a very good thing; developers can look at what each 
    provides, and determine which solves the particular issues presented in the 
    current project.
</p>

<p>
    Because of this latter point, I have my reservations about shared interfaces.
</p>

<p>
    What if a particular approach requires deviating from the shared interface in 
    order to accomplish its goals? Additionally, in order to keep the greatest
    amount of compatibility between projects, shared interfaces tend to be so
    generic that specific implementations require developers to do a ton of manual
    type checking and munging of parameters, leading to more overhead, more difficulty
    testing and maintaining, and more difficulty documenting and understanding.
</p>

<p>
    As an example, consider the following (made up) signature for a log method:
</p>

<div class="example"><pre><code language="php">
public function log($message, array $context = null);
</code></pre></div>

<p>
    What if your library supports an idea of priorities? Where would that 
    information go in the above signature -- and would that differ between 
    libraries -- would one library use the key for a completely different 
    purpose? What about logging objects -- the signature doesn't say you can't, 
    but how would I know if a specific implementation supports it, and won't 
    blow up if I do pass one? Why must the <code>$context</code> be an array -- 
    why won't any <code>Traversable</code> or <code>ArrayAccess</code> object 
    work?
</p>

<p>
    Basically, by being overly generic, the signature becomes a liability for
    those implementing the interface; it prevents meaningful interoperability
    and leads to splintering implementations.
</p>

<p><em>
    (Please note: the above is completely fictional and has no bearing
    on current proposed or accepted standards. It is a thought exercise
    only.)
</em></p>

<p>
    Furthermore, if a given project writes their own implementation of a 
    component, and it has specialized features, why would they want to typehint
    on a generic, shared interface that doesn't implement those features? This
    would be counter-intuitive, as the project would then need to either check on
    additional interfaces for the specialized capabilities, duck-type, etc. --
    all of which make for more maintenance and code.
</p>

<p>
    In summary, my primary problem with the idea of shared interfaces is that I 
    feel there is always room for new thinking and ideas in any given problem 
    space, and that this thinking should not be restricted by what already 
    exists. Secondarily, I feel that it's okay for a given project to be 
    selective about what capabilities it requires for its internal consumption 
    and consistency, and should not limit itself to a standardized interface.
</p>

<h2>But, but, SHARING</h2>

<p>
    <em>Remember, the first point I made was that I think NIH is a 
    problem.</em> How do I reconcile that with a firm stance against shared 
    interfaces?
</p>

<p>
    Easy: <a href="http://en.wikipedia.org/wiki/Bridge_pattern">bridges</a> 
    and/or <a href="http://en.wikipedia.org/wiki/Adapter_pattern">adapters</a>.
</p>

<p>
    Let's go back to that example of Framework A, its caching library, and working
    with ORM B.
</p>

<p>
    Let's assume that ORM B defines an interface for caching, and let's say it
    looks like this:
</p>

<div class="example"><pre><code language="php">
interface CacheInterface
{
    public function set($key, $data);
    public function has($key);
    public function get($key);
}
</code></pre></div>

<p>
    Furthermore, we'll assume that the expected parameter values and return types
    are documented.
</p>

<p>
    What we as a consumer of both Framework A and ORM B can do is build an 
    <em>implementation</em> of <code>CacheInterface</code> that accepts a cache
    instance from Framework A, and proxies the various interface methods to that
    instance.
</p>

<div class="example"><pre><code language="php">
class FrameworkACache implements CacheInterface
{
    protected $cache;

    public function __construct(Cache $cache)
    {
        $this->cache = $cache;
    }

    public function set($key, $data)
    {
        $item = new CacheItem($key, $data);
        $this->cache->setItem($item);
    }

    public function has($key)
    {
        return $this->cache->exists($key);
    }

    public function get($key)
    {
        $item = $this->cache->getItem($key);
        return $item->getData();
    }
}
</code></pre></div>

<p>
    Assuming your code is well-decoupled, and you're using some sort of Inversion of
    Control container, you can likely create a factory for your ORM that will grab
    the above class, with the cache injected, and inject it into the ORM instance. 
    Yes, it's a bit more work, but it's difficult to question the end result: 
    shared caching between the framework and the ORM - and no need for shared 
    interfaces, nor any need to sacrifice features within the framework or the 
    ORM.
</p>

<h2>Sharing is good, developing solutions is better</h2>

<p>
    I think the core idea of the php-fig group is sound: <em>let's all start thinking
    about how we can make it easier to operate with each other</em>. That said, my 
    thoughts on how to accomplish that goal have changed significantly, and 
    boil down to:
</p>

<ul>
    <li>Use naming conventions that will reduce collisions (i.e., use 
        per-project vendor prefixes/namespaces)</li>
    <li>Use semantic versioning</li>
    <li>Keep your installation packages segregated</li>
    <li>Have a simple, discoverable way to autoload</li>
    <li>Provide interfaces for anything that could benefit from alternate implementations</li>
    <li>Don't write code that has side-effects in the global namespace 
        (including altering PHP settings or superglobals)</li>
</ul>

<p>
    Following these principals, you can play nice with each other, while still 
    fostering innovative and differentiating solutions to shared problems.
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>PHP Master Series on Day Camp For Developers</title>
      <pubDate>Tue, 18 Dec 2012 20:24:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-12-18-php-master-series.html</link>
      <guid>http://mwop.net/blog/2012-12-18-php-master-series.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    <a href="http://blog.calevans.com">Cal Evans</a> has organized another 
    DayCamp4Developers event, this time entitled "<a 
    href="http://blog.calevans.com/2012/11/19/php-master-series-vol-1">PHP 
    Master Series, Volume 1</a>". I'm honored to be an invited speaker for this 
    first edition, where I'll be presenting my talk, "Designing Beautiful Software".
</p>

<p>
    Why would you want to participate? Well, for one, because you can interact directly
    with the various speakers during the presentations. Sure, you can likely find the slide
    decks elsewhere, or possibly even recordings. But if we all do our jobs right, we'll
    likely raise more questions than answers; if you attend, you'll get a chance to ask
    some of your questions immediately, <em>and we may even answer them!</em>
</p>

<p>
    On top of that, this is a fantastic lineup of speakers, and, frankly, not a lineup 
    I've ever participated in. In a typical conference, you'd likely see one or two of
    us, and be lucky if we weren't scheduled against each other; if you attend 
    this week, you'll get to see us all, back-to-back. 
</p>

<p>
    What else will you be doing this Friday, anyways, while <a 
    href="http://en.wikipedia.org/wiki/2012_phenomenon">you wait for the end of the 
    world?</a>
</p>

<p>
    So, do yourself a favor, and <a 
    href="http://phpmasterseriesv1.eventbrite.com/">register today</a>!
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>My ZendCon Beautiful Software Talk</title>
      <pubDate>Sat, 17 Nov 2012 13:53:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-11-17-zendcon-beautiful-software.html</link>
      <guid>http://mwop.net/blog/2012-11-17-zendcon-beautiful-software.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    Once again, I spoke at <a href="http://www.zendcon.com/">ZendCon</a> 
    this year; in talking with <a href="http://twitter.com/chwenz">Christian Wenz</a>,
    we're pretty sure that the two of us and <a href="http://andigutmans.blogspot.com">Andi</a>
    are the only ones who have spoken at all eight events.
</p>

<p>
    Unusually for me, I did not speak on a Zend Framework topic, and had
    only one regular slot (I also co-presented a Design Patterns tutorial
    with my team). That slot, however, became one of my favorite talks I've
    delivered: "Designing Beautiful Software". I've given this talk a couple
    times before, but I completely rewrote it for this conference in order 
    to better convey my core message: beautiful software is maintainable
    and extensible; writing software is a craft.
</p>

<p>
    I discovered today that not only was it recorded, but it's been <a href="http://youtu.be/mQsQ6QZ4dGg">posted
    on YouTube</a>:
</p>

<iframe width="420" height="315" src="http://www.youtube.com/embed/mQsQ6QZ4dGg" frameborder="0" allowfullscreen></iframe><p>
    I've also <a href="/slides/2012-10-25-BeautifulSoftware/BeautifulSoftware.html">posted the slides</a>.
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Zend Server, ZF2, and Page Caching</title>
      <pubDate>Mon, 05 Nov 2012 21:25:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-11-05-zend-server-caching.html</link>
      <guid>http://mwop.net/blog/2012-11-05-zend-server-caching.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    Zend Server has a very cool
    <a href="http://www.youtube.com/watch_v=i2XXn2SA5zM.html" target="_blank">Page Caching feature</a>. Basically, you can provide
    URLs or URL regular expressions, and tell Zend Server to provide full-page
    caching of those pages. This can provide a tremendous performance boost, without
    needing to change anything in your application structure; simply enable it for a
    set of pages, and sit back and relax.
</p><p style="text-align: center;">
    <img 
        style="max-width: 100%; max-height: 100%;"
        src="/images/blog/2012-11-04-Server-CachingRule.png"
        alt="Zend Server Page Caching"
        title="Zend Server Page Caching" />
</p>

<p>
    However, this feature is not entirely straight-forward when using a framework
    that provides its own routing, such as ZF2. The reason is because it assumes by
    default that each match maps to a specific file on the filesystem, and prepares
    the caching based on the actual <em>file</em> it hits. What this means for ZF2 and other
    similar frameworks is that any page that matches will return the cached version
    for the <em>first</em> match that also matches the same <em>file</em> -- i.e., <code>index.php</code> in
    ZF2. That's every page the framework handles. As an example, if I match on <code>/article/\d+</code>, it matches
    this to the file <code>index.php</code>, and then any other match that resolves to
    <code>index.php</code> gets served that same page. Not handy.
</p>

<p>
    The good part is that there's a way around this.
</p>

<p>
    When creating or modifying a caching rule, simply look for the text, "Create a
    separate cached page for each value of:" and click the "Add Parameter" button.
    Select <code>_SERVER</code> from the dropdown, and type <code>[REQUEST_URI]</code> for the value. Once
    saved, each page that matches the pattern will be cached separately.
</p>

<p>
    <img 
        style="max-width: 100%; max-height: 100%;"
        src="/images/blog/2012-11-04-Server-Caching-Request.png"
        alt="Zend Server Page Caching by Request"
        title="Zend Server Page Caching by Request" />
</p>

<p>
    Note: the <code>_SERVER</code> key may vary based on what environment/OS you're deployed
    in. Additionally, it may differ based on how you define rewrite rules -- some
    frameworks and CMS systems will append to the query string, for instance, in
    which case you may want to select the "entire query string" parameter instead of
    <code>_SERVER</code>; the point is, there's likely a way for you to configure it.
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>OpenShift, ZF2, and Composer</title>
      <pubDate>Thu, 01 Nov 2012 20:25:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-11-01-openshift-zf2-composer.html</link>
      <guid>http://mwop.net/blog/2012-11-01-openshift-zf2-composer.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
I was recently shopping around for inexpensive cloud hosting; I want to try out
a couple of ideas that may or may not have much traffic, but which aren't suited
for my VPS setup (the excellent <a href="http://servergrove.com/">ServerGrove</a>); additionally, I'm unsure how long
I will maintain these projects. My budget for this is quite small as a result;
I'm already paying for hosting, and am quite happy with it, so this is really
for experimental stuff.
</p>

<p>
I considered Amazon, Orchestra.io, and a few others, but was concerned about the
idea of a ~$50/month cost for something I'm uncertain about. 
</p>

<p>
When I asked in <a href="irc://irc.freenode.net/zftalk.dev">#zftalk.dev</a>, someone
suggested <a href="http://openshift.redhat.com/">OpenShift</a> as an idea, and
coincidentally, the very next day
<a href="http://www.zend.com/en/company/news/press/379_red-hat-expands-openshift-ecosystem-with-zend-partnership-to-offer-professional-grade-environment-for-php-developers">Zend announced a partnership with RedHat surrounding OpenShift</a>.  The stars were in alignment.
</p>

<p>
In the past month, in the few spare moments I've had (which included an
excellent OpenShift hackathon at ZendCon), I've created a quick application that
I've deployed and tested in OpenShift. These are my findings.
</p><h2>ZF2</h2>

<p>
    I didn't really have to do anything different to have <a 
    href="http://framework.zend.com/">zf2</a> work; the standard 
    <code>.htaccess</code> provided in the skeleton application worked 
    flawlessly the first time (I've worked with some cloud environments where 
    this is not the case).
</p>

<p>
The only frustration I had was the default directory structure OpenShift foists
upon us: 
</p>

<div class="example"><pre>
data/
libs/
misc/
php/
</pre></div>

<p>
This is not terrible, by any stretch. However, it's attempting to dictate the
application structure, which I'm not terribly happy with -- particularly as my
structure may vary based on the framework I'm using (or not!), and because I may
already have a project written that I simply want to deploy.
</p>

<p>
In particular, the <code>php</code> directory is galling -- it's simply the document root.
Most frameworks I've used or seen call the equivalent directory <code>public</code>, or
<code>web</code>, or <code>html</code> -- but never <code>php</code> (in large part because the only PHP file
under the document root in most frameworks is the <code>index.php</code> that acts as the
front controller). It would be nice if this were configurable.
</p>

<p>
This conflicts a bit with how a ZF2 app is structured. I ended up doing the
following:
</p>

<ul>
<li>
Removed <code>php</code> and symlinked my <code>public</code> directory to it.
</li>
<li>
Removed <code>libs</code> and symlinked my <code>vendor</code> directory to it.
</li>
<li>
Removed <code>misc</code> as I had no need to it.
</li>
</ul>

<p>
Nothing too big, thankfully -- but problematic from the perspective of, "I've
already developed this app, but now I have to make changes for it to work on a
specific cloud vendor."
</p>

<h2>Composer</h2>

<p>
    My next question was how to use <a 
    href="http://getcomposer.org/">Composer</a> during my deployment process, 
    and some some googling <a href="https://openshift.redhat.com/community/content/support-for-git-clone-on-the-server-aka-support-php-composerphar">found 
    some answers for me</a>.
</p>

<p>
Basically, I needed to create a <code>deploy</code> task that does two things:
</p>

<ul>
<li>
    Unset the <code>GIT_DIR</code> environment variable. Evidently, the build 
    process operates as part of a git hook, and since Composer often uses git 
    repositories, this can lead to problems.
</li>
<li>
    Change directory to <code>OPENSHIFT_REPO_DIR</code>, which is where the 
    application root (not document root!) lives.
</li>
</ul>

<p>
    Once I did those, I could run my normal composer installation. The 
    <code>deploy</code> task looks like this:
</p>

<div class="example"><pre><code language="bash">
#!/bin/bash
# .openshift/action_hooks/deploy
( unset GIT_DIR ; cd $OPENSHIFT_REPO_DIR ; /usr/local/zend/bin/php composer.phar install )
</code></pre></div>

<p>
This leads into my next topic.
</p>

<h2>Deployment</h2>

<p>
    First off, as you probably guessed from that last secton, there 
    <strong>are</strong> hooks for
    deployment -- it doesn't have to be simply git. I like this, as I may have
    additional things I want to do during deployment, such as retrieving and
    installing site-specific configuration files, installing Composer-defined
    dependencies (as already noted), etc.
</p>

<p>
    Over all, this is pretty seamless, but it's not without issues. I've been 
    told that some of my issues are being worked on, so those I won't bring up 
    here. The ones that were a bit strange, and which caught me by surprise, 
    though, were:
</p>

<ul>
<li>
    Though the build process creates the site build from git, your 
    <strong>submodules are not updated recursively</strong>. This tripped me 
    up, as I was using <a href="https://github.com/EvanDotPro/EdpMarkdown">EdpMarkdown</a>,
    and had installed it as a submodule. I ended up having to import it, and its
    own submodule, directly into my project so that it would work.
</li>
<li>
    I installed the <a href="http://www.mongodb.org/">MongoDB</a> cartridge. Ironically, it was not then enabled in
    Zend Server, and I had to go do this. This should be turnkey.
</li>
<li>
    <code>/usr/bin/php</code> is not the same as <code>/usr/local/zend/bin/php</code>. This makes no
    sense to me if I've installed Zend Server as my base gear. Considering
    they're different versions, this can be hugely misleading and lead to errors.
    I understand there are reasons to have both -- so simply be aware that if you
    use the Zend Server gear, your tasks likely should use
    <code>/usr/local/zend/bin/php</code>.
</li>
</ul>

<h2>The good parts?</h2>

<ul>
<li>
    <a href="https://openshift.redhat.com/community/faq/i-have-deployed-my-app-but-i-don%E2%80%99t-like-telling-people-to-visit-myapp-myusernamerhcloudcom-how-c">You 
    can alias an application to a DNS CNAME</a> -- meaning you can point your
    domain name to your OpenShift applications. Awesome!
</li>
<li>
    Simplicity of adding capabilities, such as Mongo, MySQL, Cron, and others. 
    In most cases, this is simply a "click on the button" and it's installed 
    and available.
</li>
<li>
    <a href="http://www.zend.com/en/products/server">Zend Server</a>. For 
    most PHP extensions, I can turn them on or off with a few
    mouse clicks. If I want page-level caching, I don't have to do anything to my
    application; I can simply setup some rules in the Zend Server interface and
    get on with it, and enjoy tremendous boosts to performance. I used to enjoy
    taming and tuning servers; most days anymore, I just want them to work.
</li>
<li>
    <a href="https://openshift.redhat.com/community/developers/remote-access">SSH</a> 
    access to the server, with a number of commands to which I've been given
    <code>sudoer</code> access. If you're going to sandbox somebody,
    this is a fantastic way to do it. Oh, also: SSH tunnels to services like Mongo
    and MySQL just work (via the <code>rhc-port-forward</code> command).
</li>
</ul>

<h2>Summary</h2>

<p>
Over all, I'm quite pleased. While it took me a bit to find the various
incantations I needed, the service is quite flexible. For my needs, considering
I'm doing experimental stuff, the price can't be beat (the current developer
preview is free). Considering most stuff I do will fall into this or the basic
tier, and that most cartridges do not end up counting against your alotment of
gears, the pricing ($0.05/hour) is extremely competitive. 
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>Screencasting on Linux</title>
      <pubDate>Thu, 20 Sep 2012 22:30:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-09-20-screencasting-on-linux.html</link>
      <guid>http://mwop.net/blog/2012-09-20-screencasting-on-linux.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
    I've been wanting to do screencasts on Linux for some time now, and my big
    stumbling block has been determining what tools to use.
</p>

<p>
    The <strong>tl;dr</strong>:
</p>

<ul>
    <li>
        Use <code>recordMyDesktop</code> to record video clips, but afterwards, re-encode them
        to AVI (<a href="#script">see the script I used</a>)
    </li>

    <li>
        Record audio to WAV, or convert compressed audio to WAV format afterwards.
    </li>

    <li>
        Use OpenShot to stitch clips together and layer audio and video tracks.
    </li>

    <li>
        Remember to reset the video length if you change the playback rate.
    </li>

    <li>
        Export to a Web + Vimeo profile for best results.
    </li>
</ul><h2 id="toc_1.1">Stumbling Blocks</h2>

<p>
<code>recordMyDesktop</code> is a fairly simple tool, and allows you to
record actions you're taking, and simultaneously capture audio. However, it
creates an ".ogv" (Ogg Vorbis video file) -- which is basically useless for
anybody not on Linux or FreeBSD. Additionally, I often like to record in
segments; this makes it less likely that I'll make mistakes, and, if I do, I
only need to record a small segment again, not the entire thing. <code>recordMyDesktop</code>
is only for creating screencasts, not merging them.
</p>

<p>
So, <code>recordMyDesktop</code> went into my toolbox for the purpose of recording the video
portion of my screencasts.
</p>

<p>
Which brings me to the next point: I also prefer to record the audio separately
from the screencast portion itself; this way I don't get typing sounds in the
recording, and I'm less likely to lose my train of thought as I'm speaking. 
To this end, I ended up using quite simply the "Sound Recorder" utility
(<code>gnome-sound-recorder</code>). It's not great, but with a reasonable microphone, it
gets the job done. I chose to record the audio as MP3 files.
</p>

<p>
However, this means that I now have video and audio tracks. So my toolbox needed
a utility for overlaying tracks and laying them out on a timeline independently.
</p>

<p>
I looked at a few different free tools for Linux, including <code>Avidemux</code>, <code>Cinelerra</code>,
and <code>PiTiVi</code>. <code>Avidemux</code> was not featurful enough, <code>Cinelerra</code> was too difficult to
learn (it's more of an advanced user's tool), and <code>PiTiVi</code> kept crashing on me.
So, I used the lazyweb, and tweeted a question asking what others were using --
and the unanimous response was <code>OpenShot</code> (<a href="http://www.openshotvideo.com/">http://www.openshotvideo.com/</a>).
</p>

<p>
<code>OpenShot</code> hit the sweet spot for me -- it was easy to pick up, and didn't crash.
However, I discovered problems when I exported my project to a video file. My
video, regardless of whether or not I changed the playback rate, always played
at about 2X normal speed. The audio always truncated 1 to 2 seconds before
completion.
</p>

<p>
In doing some research, I discovered:
</p>

<ul>
<li>
There are known issues with Ogg Vorbis video files. Evidently, the
   compression creates issues when re-encoding the video to another format.
</li>
<li>
Similarly, compressed audio can lead to issues such as truncation.
</li>
</ul>

<p>
Since <code>recordMyDesktop</code> doesn't allow you to select an alternate video codec, I
had to use <code>mencoder</code> to transcode it to another format. I chose AVI (Audio
Video Interleave, a video container format developed by Microsoft), as I knew it
had widespread support, using an mpeg4 codec (also widely supported). I used the
following script, found at
<a href="http://askubuntu.com/questions/17309/video-converter-ogv-to-avi-or-another-more-common-format">http://askubuntu.com/questions/17309/video-converter-ogv-to-avi-or-another-more-common-format</a>,
in order to encode my files:
</p>

<div id="script" class="example"><pre><code language="bash">
for f in *.ogv;do
newFile=${f%.*}
mencoder "$f" -o "$newFile.avi" -oac mp3lame -lameopts fast:preset=standard -ovc lavc -lavcopts vcodec=mpeg4:vbitrate=4000
done
</code></pre></div>

<p>
That solved the video issue, but I still had to solve the audio issues. I
quickly re-recorded one audio segment in Sound Recorder, and told it to use the
"Voice,Lossless (.wav type)". When I used this version of the audio, I had no
issues, other than the audio length being mis-reported within <code>OpenShot</code>. Instead
of re-recording all segments, I installed the "Sound Converter" utility (`sudo
aptitude isntall soundconverter`), and used that to convert all my MP3 files to 
WAV. Interestingly, <code>OpenShot</code> reported the audio lengths correctly this time; go
figure.
</p>

<p>
Once that was done, I was able to start stitching everything together. A few
notes, in the hopes others learn from my mistakes:
</p>

<ul>
<li>
Several times, I wanted my video to playback slower. This is very easy to do:
   right click on the clip, select "Properties", and select the "Speed" tab, and
   adjust as necessary. However, that's not all you need to do; you need to also
   re-adjust the <em>length</em> of the clip. Simply take the existing length, and
   divide it by the rate of play. As an example, if the length is 44 seconds,
   and you specify a 1/2 rate (0.5), you'd do 44 / 0.5 = 88, and set the length
   of the clip to 88s.
</li>
<li>
If you find that <code>OpenShot</code> is reporting your audio clip lengths incorrectly,
   use another tool to find the accurate length, and then set the length to
   that. I typically rounded up to the next second, as most tools were giving
   the floor value from rounding.
</li>
<li>
I chose to export using the Web + Vimeo HD profile. This worked perfectly for
   me. It created an mpeg4 file that I could preview in a browser, and then
   upload without issues. Your mileage may vary.
</li>
</ul>

<p>
Hopefully, this will serve as a reasonable guide for others foraying into
screencasts on Linux!
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
    <item>
      <title>ZF2 Modules Quickstart (Screencast)</title>
      <pubDate>Wed, 19 Sep 2012 18:10:00 +0000</pubDate>
      <link>http://mwop.net/blog/2012-09-19-zf2-module-screencast.html</link>
      <guid>http://mwop.net/blog/2012-09-19-zf2-module-screencast.html</guid>
      <author>me@mwop.net (Matthew Weier O'Phinney)</author>
      <dc:creator>Matthew Weier O'Phinney</dc:creator>
      <content:encoded><![CDATA[<p>
One of the exciting features of the newly released Zend Framework 2 is the new
module system.
</p>

<p>
While ZF1 had modules, they were difficult to manage. All resources for all
modules were initialized on each request, and bootstrapping modules was an
onerous task. Due to the difficulties, modules were never truly "plug-and-play",
and thus no ecosystem ever evolved for sharing modules.
</p>

<p>
In Zend Framework 2, we've architected the MVC from the ground up to make
modular applications as easy as possible. Within ZF2, the MVC simply cares about
events and services  and controllers are simply one kind of service. As such,
modules are primarily about telling the MVC about services and wiring event
listeners.
</p>

<p>
To give you an example, in this tutorial, I'll show you how to install the Zend
Framework 2 skeleton application, and we'll then install a module and see how
easy it is to add it to the application and then configure it.
</p><p>
To keep things simple, I'm using a unix-like environment. As such, if you are on
Windows, you may not have the same command-line tools available. If you are in
such a situation, perhaps try this inside a Linux virtual machine.
</p>

<iframe src="http://player.vimeo.com/video/49775540" 
width="500" height="281" frameborder="0" 
webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>
<p><a href="http://vimeo.com/49775540">Zend Framework 2 Module Quickstart</a></p>

<p>
Let's start by creating a new project. We'll execute a few commands to download
a skeleton application archive and extract it.
</p>

<div class="example"><pre><code language="bash">
% mkdir newproject
% cd newproject
% wget https://github.com/zendframework/ZendSkeletonApplication/tarball/master \
    -O ZendSkeletonApplication.tgz
% tar xzf ZendSkeletonApplication.tgz --strip-components=1
</code></pre></div>

<p>
    The Zend Framework skeleton application can be downloaded directly off of
    <a href="https://github.com">GitHub</a>. I'm showing using the download 
    from master, but you can also download a tarball or zipball for individual 
    tags as well. Because the download URL does not include an extension, I use 
    the <code>-O</code> switch to tell <code>wget</code> what filename to save to.
</p>

<p>
    <code>tar</code> has a nice option, `<code>--strip-components</code>`, which allows you to tell it to
    descend a certain number of levels deep into the archive when deflating. Since I
    know the tarball has a top-level directory named after the repository and a
    sha1, I'm simply telling <code>tar</code> to skip that and give me the contents of its
    child directory.
</p>

<p>
    At this point you have the skeleton application, but it has no dependencies 
     not even Zend Framework itself! Let's rectify that situation. We'll use the
    dependency management tool <a href="https://getcomposer.org/">Composer</a> 
    to do this. We include the Composer phar file within the skeleton 
    application to make this fairly easy. Simply execute the following:
</p>

<div class="example"><pre><code language="bash">
% php composer.phar install
</code></pre></div>

<p>
    You may get a notice indicating that the composer version is older, and to 
    run "self-update"; you can ignore that for now.
</p>

<p>
    If all goes well, you should now have Zend Framework installed. Let's test 
    it out. I'm going to use the built-in web server in PHP 5.4 to demonstrate.
</p>

<div class="example"><pre><code language="bash">
% cd public
% php -S localhost:8080
</code></pre></div>

<p>
    If I browse to <code>http://localhost:8080</code> I should now see the 
    landing page for the skeleton application.
</p>

<img src="/images/screencasts/2012-09-19-zf2-module-screencast-01-zsa.png" style="width: 100%; height: 100%;" />

<p>
    Let's add a module to the application. Many sites require a contact form. 
    I've written one as a module some time ago, and called it <a 
    href="https://github.com/weierophinney/PhlyContact">PhlyContact</a>. To install
    it, I'll edit my project's <code>composer.json</code> and tell it about that 
    dependency:
</p>

<div class="example"><pre><code language="json">
{
    "require": {
        "php": "&gt;=5.3.3",
        "zendframework/zendframework": "dev-master",
        "phly/phly-contact": "dev-master"
    }
}
</code></pre></div>
    
<p>
    I know the name of the component from <a 
    href="http://packagist.org/">http://packagist.org/</a>, and I'm telling
    Composer that I want to use whatever the latest version is on its master 
    branch on GitHub. I happen to also know that PhlyContact requires a dev-master 
    version of Zend Framework, so I'll alter that dependency for now.
</p>

<p>
    Now, we need to tell composer to update our dependencies.
</p>

<div class="example"><pre><code language="bash">
% php composer.phar update
</code></pre></div>
    
<p>
    After executing the command, we should now see that it has installed; this 
    may take a little while.
</p>

<p>
    You need to inform the application about the module. This is so that we don't
    have to perform expensive file-system scanning operations, but also to make it
    explicit in your code what modules you're actually using. Enabling a module is
    usually as easy as adding an entry to <code>config\application.config.php</code>:
</p>

<div class="example"><pre><code language="php">
'modules' => array(
    'Application',
    'PhlyContact',
),
</code></pre></div>

<p>
    This particular module provides some reasonable defaults. In particular, it uses
    a CAPTCHA adapter that doesn't require additional configuration, and assumes
    that you will want to use the default <code>Sendmail</code> mail transport. As such, we can
    simply browse to it now. I happen to know that the module defines a <code>/contact</code>
    end point. Let's fire up our PHP web server again, and browse to that URL.
</p>

<div class="example"><pre><code language="bash">
% cd public
% php -S localhost:8080
</code></pre></div>

<img src="/images/screencasts/2012-09-19-zf2-module-screencast-02-contact.png" style="width: 100%; height: 100%;" />

<p>
    It just works!
</p>

<p>
    One philosophy we have for distributable modules in Zend Framework 2 is 
    that you should not need to touch the code in modules you install in your application.
    Instead, you should be able to configure and override behavior within the
    application configuration or in your application's site-specific modules. Let's
    alter the contact module to:
</p>

<ul>
    <li>first, change the URL it responds to, and</li>
    <li>second, use the "file" mail transport.</li>
</ul>

<p>
    Let's look at the default configuration. I'll browse to
    <code>vendor/phly/phly-contact/config/</code> and look at the <code>module.config.php</code> file.
</p>

<div class="example"><pre><code language="php">
return array(
    'phly_contact' => array(
        'captcha' => array(
            'class' => 'dumb',
        ),
        'form' => array(
            'name' => 'contact',
        ),
        'mail_transport' => array(
            'class' => 'Zend\Mail\Transport\Sendmail',
            'options' => array(
            )
        ),
        'message' => array(
            /*
            'to' => array(
                'EMAIL HERE' => 'NAME HERE',
            ),
            'sender' => array(
                'address' => 'EMAIL HERE',
                'name'    => 'NAME HERE',
            ),
            'from' => array(
                'EMAIL HERE' => 'NAME HERE',
            ),
             */
        ),
    ),

    /* ... */

    'router' => array(
        'routes' => array(
            'contact' => array(
                'type' => 'Literal',
                'options' => array(
                    'route' => '/contact',
                    'defaults' => array(
                        '__NAMESPACE__' => 'PhlyContact\Controller',
                        'controller'    => 'Contact',
                        'action'        => 'index',
                    ),
                ),
                'may_terminate' => true,
                'child_routes' => array(
                    'process' => array(
                        'type' => 'Literal',
                        'options' => array(
                            'route' => '/process',
                            'defaults' => array(
                                'action' => 'process',
                            ),
                        ),
                    ),
                    'thank-you' => array(
                        'type' => 'Literal',
                        'options' => array(
                            'route' => '/thank-you',
                            'defaults' => array(
                                'action' => 'thank-you',
                            ),
                        ),
                    ),
                ),
            ),
        ),
    ),
    /* ... */
);
</code></pre></div>

<p>
    Okay, that's interesting. I can define the captcha and options to use, the 
    name of the contact form, the mail transport I want to use, and even who the email 
    is sent from and who it goes to. In addition, it defines some routes.
</p>

<p>
    I'll create a new file, <code>config/autoload/phly-contact.local.php</code>. This is a
    local configuration file that will not be checked into my version control
    system. Now, let's add some configuration. First, I'll configure my mail
    transport.
</p>

<div class="example"><pre><code language="php">
return array(
    'phly_contact' => array(
        'mail_transport' => array(
            'class'   => 'Zend\Mail\Transport\File',
            'options' => array(
                'path' => 'data/mail/',
            ),
        ),
    ),
);
</code></pre></div>

<p>
    I'm telling the module to use the <code>File</code> mail transport, and telling the transport
    where I want messages written. By default, Zend Framework calls <code>chdir()</code> to
    change directory to the project root, so I can reference a directory relative to
    that. I'm simply going to write to a <code>data/mail/</code> directory. Let's create that,
    and make it world-writable for now to ensure the web server can write to it. (In
    production, you'd only want it writable by the web server user.)
</p>

<div class="example"><pre><code language="bash">
% mkdir -p data/mail
% chmod a+rwX data/mail
</code></pre></div>

<p>
    Now, let's change the base URL the contact form responds to; I want it to
    respond to <code>/contact-us</code>. Another principle of re-usable modules in ZF2 is that
    we recommend creating tree routes for each module, with the root of the tree
    being a literal route. This makes it easy to alter the base for routing, without
    needing to redefine all the routes in the module.
</p>

<p>
    I'll add the following to my local configuration, then. I'll simply override the
    parent route for my module, named "contact", and point it at a different URL.
</p>

<div class="example"><pre><code language="php">
'router' => array(
    'routes' => array(
        'contact' => array(
            'options' => array(
                'route' => '/contact-us',
            ),
        ),
    ),
),
</code></pre></div>

<p>
    Let's see if all this worked! Once again, I'll fire up PHP's built-in web
    server.
</p>

<div class="example"><pre><code language="bash">
% cd public
% php -S localhost:8080
</code></pre></div>

<p>
    Now, let's browse to <code>http://localhost:8080/contact-us</code> -- looks good! Just as an
    experiment, let's try the previously configured URL,
    <code>http://localhost:8080/contact</code>. We get a 404 now!
</p>

<img src="/images/screencasts/2012-09-19-zf2-module-screencast-03-config.png" style="width: 100%; height: 100%;" />
<img src="/images/screencasts/2012-09-19-zf2-module-screencast-04-404.png" style="width: 100%; height: 100%;" />

<p>
    Now, let's submit the form. I'll fill in some information; it's asking for my
    email address, a subject line, and a message, as well as for me to solve a
    simple CAPTCHA. Once I've done all that, I can send it.
</p>

<p>
    If all is well, we should now have a mail file in our data directory. Let's
    check.
</p>

<div class="example"><pre><code language="bash">
% ls -l data/mail/
</code></pre></div>

<p>
    And now let's look at it.
</p>

<div class="example"><pre><code language="bash">
% cat data/mail/ZendMail_1347989389_1009740165.tmp
Date: Tue, 18 Sep 2012 12:29:49 -0500
From: me@mwop.net
Reply-To: me@mwop.net
Subject: [Contact Form] Suspense!

Suspenseful, isn't it?
</code></pre></div>
    
<p>
    Looks good!
</p>

<p>
    Zend Framework 2 provides a wonderful modular architecture that will enable an
    ecosystem of 3rd party modules that should save you time and energy when
    developing your applications. I've demonstrated a simple one, a contact form,
    but many, many more already exist, and with a stable release now available, you
    should see that number grow. This is truly a wonderful step forward for
    developers, and I hope you find it as exciting as I do.
</p>]]></content:encoded>
      <slash:comments>0</slash:comments>
    </item>
  </channel>
</rss>
